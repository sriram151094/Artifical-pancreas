{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import statsmodels.datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, cohen_kappa_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_file(filepath):\n",
    "    d = []\n",
    "    with open(filepath) as csvfile:\n",
    "        areader = csv.reader(csvfile)\n",
    "        max_elems = 0\n",
    "        for row in areader:\n",
    "            if max_elems < len(row): max_elems = len(row)\n",
    "        csvfile.seek(0)\n",
    "        for i, row in enumerate(areader):\n",
    "            # fix my csv by padding the rows\n",
    "            d.append(row + [\"\" for x in range(max_elems-len(row))])\n",
    "\n",
    "    df = pd.DataFrame(d)\n",
    "    return df\n",
    "\n",
    "MealData1 = read_csv_file('./mealData1.csv');\n",
    "MealData2 = read_csv_file('./mealData2.csv');\n",
    "MealData3 = read_csv_file('./mealData3.csv');\n",
    "MealData4 = read_csv_file('./mealData4.csv');\n",
    "MealData5 = read_csv_file('./mealData5.csv');\n",
    "#Importing No Meal Data\n",
    "\n",
    "Nomeal1 = read_csv_file('./Nomeal1.csv')\n",
    "Nomeal2 = read_csv_file('./Nomeal2.csv')\n",
    "Nomeal3 = read_csv_file('./Nomeal3.csv')\n",
    "Nomeal4 = read_csv_file('./Nomeal4.csv')\n",
    "Nomeal5 = read_csv_file('./Nomeal5.csv')\n",
    "\n",
    "df_list = [MealData1, MealData2, MealData3, MealData4, MealData5, Nomeal1, Nomeal2, Nomeal3, Nomeal4, Nomeal5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51, 31) (51, 31) (51, 31) (51, 31) (51, 31)\n",
      "(51, 30) (51, 30) (51, 30) (51, 30) (51, 30)\n",
      "Total no of records::  510\n"
     ]
    }
   ],
   "source": [
    "print(MealData1.shape, MealData2.shape, MealData3.shape, MealData4.shape, MealData5.shape)\n",
    "print(Nomeal1.shape, Nomeal2.shape, Nomeal3.shape, Nomeal4.shape, Nomeal5.shape)\n",
    "print(\"Total no of records:: \", MealData1.shape[0]+MealData2.shape[0]+MealData3.shape[0]+MealData4.shape[0]+MealData5.shape[0]\n",
    "     + Nomeal1.shape[0] + Nomeal2.shape[0] + Nomeal3.shape[0] + Nomeal4.shape[0] + Nomeal5.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idx = 0\n",
    "for df in df_list:\n",
    "    for i in range(len(df.iloc[0])):\n",
    "        df[i] = pd.to_numeric(df[i],errors='coerce')\n",
    "    if df_idx == 0:\n",
    "        MealData1 = df\n",
    "    if df_idx == 1:\n",
    "        MealData2 = df\n",
    "    if df_idx == 2:\n",
    "        MealData3 = df\n",
    "    if df_idx == 3:\n",
    "        MealData4 = df\n",
    "    if df_idx == 4:\n",
    "        MealData5 = df\n",
    "    if df_idx == 5:\n",
    "        Nomeal1 = df\n",
    "    if df_idx == 6:\n",
    "        Nomeal2 = df\n",
    "    if df_idx == 7:\n",
    "        Nomeal3 = df\n",
    "    if df_idx == 8:\n",
    "        Nomeal4 = df\n",
    "    if df_idx == 9:\n",
    "        Nomeal5 = df\n",
    "    df_list[df_idx] = df\n",
    "    df_idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 31st column\n",
    "\n",
    "df_idx = 0\n",
    "for df in df_list:\n",
    "    if df_idx < 5:\n",
    "        df = df.drop(df.columns[[30]], axis = 1)\n",
    "        \n",
    "        if df_idx == 0:\n",
    "            MealData1 = df\n",
    "        if df_idx == 1:\n",
    "            MealData2 = df\n",
    "        if df_idx == 2:\n",
    "            MealData3 = df\n",
    "        if df_idx == 3:\n",
    "            MealData4 = df\n",
    "        if df_idx == 4:\n",
    "            MealData5 = df\n",
    "        df_list[df_idx] = df\n",
    "        \n",
    "    df_idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MealData1 = MealData1.iloc[:, ::-1]\n",
    "MealData2 = MealData2.iloc[:, ::-1]\n",
    "MealData3 = MealData3.iloc[:, ::-1]\n",
    "MealData4 = MealData4.iloc[:, ::-1]\n",
    "MealData5 = MealData5.iloc[:, ::-1]\n",
    "\n",
    "\n",
    "Nomeal1 = Nomeal1.iloc[:, ::-1]\n",
    "Nomeal2 = Nomeal2.iloc[:, ::-1]\n",
    "Nomeal3 = Nomeal3.iloc[:, ::-1]\n",
    "Nomeal4 = Nomeal4.iloc[:, ::-1]\n",
    "Nomeal5 = Nomeal5.iloc[:, ::-1]\n",
    "\n",
    "df_list = [MealData1, MealData2, MealData3, MealData4, MealData5, Nomeal1, Nomeal2, Nomeal3, Nomeal4, Nomeal5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idx = 0\n",
    "for df in df_list:\n",
    "    for i in range(len(df)):\n",
    "        df.loc[[i]] = df.loc[[i]].interpolate(axis=1, limit=None, limit_direction='both')\n",
    "    if df_idx == 0:\n",
    "        MealData1 = df\n",
    "    if df_idx == 1:\n",
    "        MealData2 = df\n",
    "    if df_idx == 2:\n",
    "        MealData3 = df\n",
    "    if df_idx == 3:\n",
    "        MealData4 = df\n",
    "    if df_idx == 4:\n",
    "        MealData5 = df\n",
    "    if df_idx == 5:\n",
    "        Nomeal1 = df\n",
    "    if df_idx == 6:\n",
    "        Nomeal2 = df\n",
    "    if df_idx == 7:\n",
    "        Nomeal3 = df\n",
    "    if df_idx == 8:\n",
    "        Nomeal4 = df\n",
    "    if df_idx == 9:\n",
    "        Nomeal5 = df\n",
    "    df_list[df_idx] = df\n",
    "    df_idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idx = 0\n",
    "for df in df_list:\n",
    "    df = df.dropna()\n",
    "    df = df.reset_index(drop=True)\n",
    "    if df_idx == 0:\n",
    "        MealData1 = df\n",
    "    if df_idx == 1:\n",
    "        MealData2 = df\n",
    "    if df_idx == 2:\n",
    "        MealData3 = df\n",
    "    if df_idx == 3:\n",
    "        MealData4 = df\n",
    "    if df_idx == 4:\n",
    "        MealData5 = df\n",
    "    if df_idx == 5:\n",
    "        Nomeal1 = df\n",
    "    if df_idx == 6:\n",
    "        Nomeal2 = df\n",
    "    if df_idx == 7:\n",
    "        Nomeal3 = df\n",
    "    if df_idx == 8:\n",
    "        Nomeal4 = df\n",
    "    if df_idx == 9:\n",
    "        Nomeal5 = df\n",
    "    df_list[df_idx] = df\n",
    "    df_idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 30) (50, 30) (49, 30) (51, 30) (51, 30)\n",
      "(51, 30) (50, 30) (43, 30) (51, 30) (50, 30)\n",
      "Total no of records::  494\n"
     ]
    }
   ],
   "source": [
    "print(MealData1.shape, MealData2.shape, MealData3.shape, MealData4.shape, MealData5.shape)\n",
    "print(Nomeal1.shape, Nomeal2.shape, Nomeal3.shape, Nomeal4.shape, Nomeal5.shape)\n",
    "print(\"Total no of records:: \", MealData1.shape[0]+MealData2.shape[0]+MealData3.shape[0]+MealData4.shape[0]+MealData5.shape[0]\n",
    "     + Nomeal1.shape[0] + Nomeal2.shape[0] + Nomeal3.shape[0] + Nomeal4.shape[0] + Nomeal5.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "#Scaling CGM Data\n",
    "MealData1Scaled = scaler.fit_transform(MealData1)\n",
    "MealData2Scaled = scaler.fit_transform(MealData2)\n",
    "MealData3Scaled = scaler.fit_transform(MealData3)\n",
    "MealData4Scaled = scaler.fit_transform(MealData4)\n",
    "MealData5Scaled = scaler.fit_transform(MealData5)\n",
    "\n",
    "no_meal1_scaled = scaler.fit_transform(Nomeal1)\n",
    "no_meal2_scaled = scaler.fit_transform(Nomeal2)\n",
    "no_meal3_scaled = scaler.fit_transform(Nomeal3)\n",
    "no_meal4_scaled = scaler.fit_transform(Nomeal4)\n",
    "no_meal5_scaled = scaler.fit_transform(Nomeal5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature FFT Calculation\n",
    "top_fft_features = []\n",
    "\n",
    "df_list = [MealData1Scaled, MealData2Scaled, MealData3Scaled, MealData4Scaled, MealData5Scaled,\n",
    "          no_meal1_scaled, no_meal2_scaled, no_meal3_scaled, no_meal4_scaled, no_meal5_scaled]\n",
    "i = 0\n",
    "for df in zip(df_list):\n",
    "    if len(top_fft_features) == 0:\n",
    "        top_fft_features = np.abs(np.fft.fft(np.flip(df)))\n",
    "    else:\n",
    "        top_fft_features = np.concatenate((top_fft_features, np.abs(np.fft.fft(np.flip(df)))), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DWT Feature Calculation\n",
    "import pywt\n",
    "def calc_feature_dwt(df):\n",
    "    cA, cB = pywt.dwt(df, 'haar')\n",
    "    cA_threshold = pywt.threshold(cA, np.std(cA)/2, mode='soft')\n",
    "    cB_threshold = pywt.threshold(cB, np.std(cB)/2, mode='soft')\n",
    " \n",
    "    \n",
    "    reconstructed_signal = pywt.idwt(cA_threshold, cB_threshold, 'haar')\n",
    "\n",
    "    feature_dwt_top8 = cA[:,:-8] #sorted in Ascending\n",
    "\n",
    "    return feature_dwt_top8\n",
    "\n",
    "df_list = [MealData1Scaled, MealData2Scaled, MealData3Scaled, MealData4Scaled, MealData5Scaled,\n",
    "          no_meal1_scaled, no_meal2_scaled, no_meal3_scaled, no_meal4_scaled, no_meal5_scaled]\n",
    "top_dwt_features = []\n",
    "for df in df_list:\n",
    "    if len(top_dwt_features) == 0:\n",
    "        top_dwt_features = calc_feature_dwt(df)\n",
    "    else:\n",
    "        top_dwt_features = np.concatenate((top_dwt_features, calc_feature_dwt(df)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coeffecient of Variation Feature Calculation\n",
    "feature_COV = []\n",
    "\n",
    "df_list = [MealData1Scaled, MealData2Scaled, MealData3Scaled, MealData4Scaled, MealData5Scaled,\n",
    "          no_meal1_scaled, no_meal2_scaled, no_meal3_scaled, no_meal4_scaled, no_meal5_scaled]\n",
    "for df in df_list:\n",
    "    for i in range(len(df)):\n",
    "        feature_COV.append(np.mean(df[i]) / np.std(df[i]))\n",
    "\n",
    "feature_COV_ = np.asarray(feature_COV)\n",
    "\n",
    "feature_COV_WO_nan = feature_COV_[np.isnan(feature_COV_) == False]\n",
    "feature_COV_WO_nan.sort()\n",
    "mean_with_threshold = np.mean(feature_COV_WO_nan[0:len(feature_COV_WO_nan)-1])\n",
    "mean_with_threshold\n",
    "\n",
    "# feature_COV_.replace(np.nan,mean_with_threshold)\n",
    "feature_COV_[feature_COV_ > 200] = mean_with_threshold\n",
    "for x in range(len(feature_COV_)):\n",
    "    if np.isnan(feature_COV_[x]):\n",
    "        feature_COV_[x] = mean_with_threshold\n",
    "# type(feature_COV_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(494,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.integrate import simps\n",
    "feature_auc = []\n",
    "\n",
    "for df in df_list:\n",
    "    for x in simps(df[:,::-1], dx = 5):\n",
    "        feature_auc.append(x)\n",
    "\n",
    "\n",
    "feature_auc = np.asarray(feature_auc)\n",
    "feature_auc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(494,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Windowed Entropy Feature \n",
    "import scipy\n",
    "output_entropy = []\n",
    "\n",
    "y = []\n",
    "ordered_cgm = []\n",
    "df_list = [MealData1, MealData2, MealData3, MealData4, MealData5,\n",
    "          Nomeal1, Nomeal2, Nomeal3, Nomeal4, Nomeal5]\n",
    "for df in df_list:\n",
    "    for j in range(len(df)):\n",
    "        temp = []\n",
    "        temp1 = []\n",
    "        c = df.iloc[j]\n",
    "        for m in range(len(c)-1,-1,-1):\n",
    "            temp.append(c[m])\n",
    "        y_array = np.array(temp)\n",
    "        ordered_cgm.append(y_array)\n",
    "        \n",
    "for i in range(len(ordered_cgm)):\n",
    "    entropy_arr = []\n",
    "    for j in range(1, 30, 5):\n",
    "        s = scipy.stats.entropy(np.asarray(ordered_cgm)[i, j:j+5])\n",
    "        #print(s)\n",
    "        entropy_arr.append(s)\n",
    "    output_entropy.append(np.amin(np.asarray(entropy_arr)))\n",
    "\n",
    "output_entropy = np.asarray(output_entropy)\n",
    "output_entropy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lognorm_mean_list = []\n",
    "lognorm_std_list = []\n",
    "df_list = [MealData1Scaled, MealData2Scaled, MealData3Scaled, MealData4Scaled, MealData5Scaled,\n",
    "          no_meal1_scaled, no_meal2_scaled, no_meal3_scaled, no_meal4_scaled, no_meal5_scaled]\n",
    "\n",
    "for df in df_list:\n",
    "    for i in range(len(df)):\n",
    "        x = df[i]\n",
    "        x[x == 0] = np.mean(x)\n",
    "        mu = np.mean(x) \n",
    "        sigma = np.std(x)\n",
    "\n",
    "        x_exp = x\n",
    "        mu_exp = np.exp(mu)\n",
    "        sigma_exp = np.exp(sigma)\n",
    "\n",
    "        fitting_params_lognormal = scipy.stats.lognorm.fit(x_exp, floc=0, scale=mu_exp)\n",
    "        lognorm_dist_fitted = scipy.stats.lognorm(*fitting_params_lognormal)\n",
    "        t = np.linspace(np.min(x_exp), np.max(x_exp), 100)\n",
    "\n",
    "        lognorm_dist = scipy.stats.lognorm(s=sigma, loc=0, scale=np.exp(mu))\n",
    "        lognorm_mean_list.append(lognorm_dist.mean())\n",
    "        lognorm_std_list.append(lognorm_dist.std())\n",
    "\n",
    "\n",
    "lognorm_std_list = np.asarray(lognorm_std_list)\n",
    "lognorm_mean_list = np.asarray(lognorm_mean_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(494, 19)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating the feature Matrix\n",
    "\n",
    "Feature_Matrix = np.hstack((top_fft_features[0][:,1:9], top_dwt_features[:,1:7], output_entropy[:, None], feature_COV_[:, None],\n",
    "                            lognorm_mean_list[:,None], lognorm_std_list[:,None], feature_auc[:, None]))\n",
    "Feature_Matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scaled_Feature_Matrix = scaler.fit_transform(Feature_Matrix)\n",
    "New_Scaled_Feature_Matrix = np.nan_to_num(Scaled_Feature_Matrix)\n",
    "\n",
    "Scaled_Feature_Matrix_ = np.asmatrix(New_Scaled_Feature_Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA for the Feature matrix of Training data\n",
    "import pickle\n",
    "pca = PCA(n_components=10)\n",
    "# Meal-data\n",
    "reduced_meal_matrix = pca.fit_transform(New_Scaled_Feature_Matrix[0:249])\n",
    "# Non-meal data\n",
    "reduced_no_meal_matrix = pca.transform(New_Scaled_Feature_Matrix[249:])\n",
    "\n",
    "filename = 'pca_model.sav'\n",
    "pickle.dump(pca,open(filename,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Feature_M = np.vstack((reduced_meal_matrix,reduced_no_meal_matrix))\n",
    "Final_Feature_M.shape\n",
    "PCA_Feature_Matrix = Final_Feature_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(494,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_M = []\n",
    "for i in range(len(reduced_meal_matrix)):\n",
    "    label_M.append(1)\n",
    "for i in range(len(reduced_no_meal_matrix)):\n",
    "    label_M.append(0)\n",
    "    \n",
    "label_M_ = np.asarray(label_M)\n",
    "label_M_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[161, 84], [97, 152]]\n",
      "Accuracy : 0.76\n",
      "Precision : 0.76\n",
      "Recall : 0.7916666666666666\n",
      "F1 score : 0.76\n"
     ]
    }
   ],
   "source": [
    "#Importing sklearn libraries for train and test\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#Array to store accuracy for each K Fold testing\n",
    "score_knn = []\n",
    "\n",
    "# confusion matrix\n",
    "confusion_knn=[]\n",
    "\n",
    "#storing precision\n",
    "precision_knn=[]\n",
    "\n",
    "#recall\n",
    "recall_knn=[]\n",
    "\n",
    "#f1 score\n",
    "f1_score_knn=[]\n",
    "\n",
    "# result  matrix\n",
    "d=[]\n",
    "result = [[0,0],[0,0]]\n",
    "\n",
    "#Splitting out features and label dataset into train and test sets\n",
    "kf_ = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "#Creating model with n=5\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "for train_index, test_index in kf_.split(Final_Feature_M):\n",
    "    X_train_knn, X_test_knn = Final_Feature_M[train_index], Final_Feature_M[test_index]\n",
    "    Y_train_knn, Y_test_knn = label_M_[train_index], label_M_[test_index]\n",
    "    \n",
    "    #Training model using the train split data from above\n",
    "    knn.fit(X_train_knn,Y_train_knn)\n",
    "    preds = knn.predict(X_test_knn)\n",
    "    \n",
    "    d=confusion_matrix(Y_test_knn, preds)\n",
    "    \n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            result[i][j] = (result[i][j] + d[i][j])\n",
    "    # Precision \n",
    "    a=precision_score(Y_test_knn, preds)\n",
    "    precision_knn.append(a)\n",
    "    \n",
    "    # Recall\n",
    "    b=recall_score(Y_test_knn, preds)\n",
    "    recall_knn.append(b)\n",
    "\n",
    "    # F1 score\n",
    "    c=f1_score(Y_test_knn, preds)\n",
    "    f1_score_knn.append(c)\n",
    "    \n",
    "    #Pickle Code Starts\n",
    "    knn_file = 'knn_model.sav'\n",
    "    pickle.dump(knn,open(knn_file,'wb'))\n",
    "\n",
    "    score_knn.append(knn.score(X_test_knn,Y_test_knn))\n",
    "\n",
    "Final_Accuracy_knn = np.max(score_knn)\n",
    "Final_precision_knn=np.max(precision_knn)\n",
    "Final_recall_knn=np.max(recall_knn)\n",
    "Final_f1_score_knn=np.max(f1_score_knn)\n",
    "print(result)\n",
    "print(\"Accuracy : %s\"%(Final_Accuracy_knn))  \n",
    "print(\"Precision : %s\"%(Final_precision_knn))\n",
    "print(\"Recall : %s\"%(Final_recall_knn))\n",
    "print(\"F1 score : %s\"%(Final_f1_score_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[198, 47], [117, 132]]\n",
      "Accuracy : 0.7551020408163265\n",
      "Precision : 0.8888888888888888\n",
      "Recall : 0.6666666666666666\n",
      "F1 score : 0.7346938775510203\n"
     ]
    }
   ],
   "source": [
    "# SVM MODEL CODE\n",
    "\n",
    "#Importing sklearn libraries for train and test\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import svm\n",
    "\n",
    "#Array to store accuracy for each K Fold testing\n",
    "score_svm = []\n",
    "\n",
    "# confusion matrix\n",
    "confusion_svm=[]\n",
    "\n",
    "#storing precision\n",
    "precision_svm=[]\n",
    "\n",
    "#recall\n",
    "recall_svm=[]\n",
    "\n",
    "#f1 score\n",
    "f1_score_svm=[]\n",
    "\n",
    "# result  matrix\n",
    "d=[]\n",
    "result = [[0,0],[0,0]]\n",
    "\n",
    "#Splitting out features and label dataset into train and test sets\n",
    "kf_ = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "#Creating SVM Classifier\n",
    "svm = svm.SVC(kernel='sigmoid', gamma='auto')\n",
    "\n",
    "for train_index, test_index in kf_.split(PCA_Feature_Matrix):\n",
    "    X_train_svm, X_test_svm = PCA_Feature_Matrix[train_index], PCA_Feature_Matrix[test_index]\n",
    "    Y_train_svm, Y_test_svm = label_M_[train_index], label_M_[test_index]\n",
    "    \n",
    "    #Training model using the train split data from above\n",
    "    svm.fit(X_train_svm,Y_train_svm)\n",
    "    preds = svm.predict(X_test_svm)\n",
    "    \n",
    "    d=confusion_matrix(Y_test_svm, preds)\n",
    "    \n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            result[i][j] = (result[i][j] + d[i][j])\n",
    "    # Precision \n",
    "    a=precision_score(Y_test_svm, preds)\n",
    "    precision_svm.append(a)\n",
    "    \n",
    "    # Recall\n",
    "    b=recall_score(Y_test_svm, preds)\n",
    "    recall_svm.append(b)\n",
    "\n",
    "    # F1 score\n",
    "    c=f1_score(Y_test_svm, preds)\n",
    "    f1_score_svm.append(c)\n",
    "    \n",
    "    #Pickle Code Starts\n",
    "    svm_file = 'svm_model.sav'\n",
    "    pickle.dump(svm,open(svm_file,'wb'))\n",
    "    score_svm.append(svm.score(X_test_svm,Y_test_svm))\n",
    "\n",
    "Final_Accuracy_svm = np.max(score_svm)\n",
    "Final_precision_svm=np.max(precision_svm)\n",
    "Final_recall_svm=np.max(recall_svm)\n",
    "Final_f1_score_svm=np.max(f1_score_svm)\n",
    "print(result)\n",
    "print(\"Accuracy : %s\"%(Final_Accuracy_svm))  \n",
    "print(\"Precision : %s\"%(Final_precision_svm))\n",
    "print(\"Recall : %s\"%(Final_recall_svm))\n",
    "print(\"F1 score : %s\"%(Final_f1_score_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[190, 55], [110, 139]]\n",
      "Accuracy : 0.8\n",
      "Precision : 0.9411764705882353\n",
      "Recall : 0.7419354838709677\n",
      "F1 score : 0.7666666666666667\n"
     ]
    }
   ],
   "source": [
    "# NAIVE BAYES CODE SNIPPET\n",
    "\n",
    "#Importing sklearn libraries for train and test\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#Array to store accuracy for each K Fold testing\n",
    "score_nb = []\n",
    "\n",
    "# confusion matrix\n",
    "confusion_nb=[]\n",
    "\n",
    "#storing precision\n",
    "precision_nb=[]\n",
    "\n",
    "#recall\n",
    "recall_nb=[]\n",
    "\n",
    "#f1 score\n",
    "f1_score_nb=[]\n",
    "\n",
    "# result  matrix\n",
    "d=[]\n",
    "result = [[0,0],[0,0]]\n",
    "\n",
    "#Splitting out features and label dataset into train and test sets\n",
    "kf_ = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "#Creating Gaussian Classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "for train_index, test_index in kf_.split(PCA_Feature_Matrix):\n",
    "    X_train_nb, X_test_nb = PCA_Feature_Matrix[train_index], PCA_Feature_Matrix[test_index]\n",
    "    Y_train_nb, Y_test_nb = label_M_[train_index], label_M_[test_index]\n",
    "    \n",
    "    #Training model using the train split data from above\n",
    "    gnb.fit(X_train_nb,Y_train_nb)\n",
    "    preds = gnb.predict(X_test_nb)\n",
    "    \n",
    "    d=confusion_matrix(Y_test_nb, preds)\n",
    "    \n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            result[i][j] = (result[i][j] + d[i][j])\n",
    "    # Precision \n",
    "    a=precision_score(Y_test_nb, preds)\n",
    "    precision_nb.append(a)\n",
    "    \n",
    "    # Recall\n",
    "    b=recall_score(Y_test_nb, preds)\n",
    "    recall_nb.append(b)\n",
    "\n",
    "    # F1 score\n",
    "    c=f1_score(Y_test_nb, preds)\n",
    "    f1_score_nb.append(c)\n",
    "    \n",
    "    #Pickle Code Starts\n",
    "    gnb_file = 'gnb_model.sav'\n",
    "    pickle.dump(gnb,open(gnb_file,'wb'))\n",
    "    score_nb.append(gnb.score(X_test_nb,Y_test_nb))\n",
    "\n",
    "Final_Accuracy_nb = np.max(score_nb)\n",
    "Final_precision_nb=np.max(precision_nb)\n",
    "Final_recall_nb=np.max(recall_nb)\n",
    "Final_f1_score_nb=np.max(f1_score_nb)\n",
    "print(result)\n",
    "print(\"Accuracy : %s\"%(Final_Accuracy_nb))  \n",
    "print(\"Precision : %s\"%(Final_precision_nb))\n",
    "print(\"Recall : %s\"%(Final_recall_nb))\n",
    "print(\"F1 score : %s\"%(Final_f1_score_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[155, 90], [90, 159]]\n",
      "Accuracy : 0.7142857142857143\n",
      "Precision : 0.7692307692307693\n",
      "Recall : 0.88\n",
      "F1 score : 0.7457627118644068\n"
     ]
    }
   ],
   "source": [
    "# DECISION TREE CODE\n",
    "\n",
    "#importing the librarries for train and test and model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#Array to store Accuracy for each K FOld testing\n",
    "score_dt = []\n",
    "\n",
    "# confusion matrix\n",
    "confusion_dt=[]\n",
    "\n",
    "#storing precision\n",
    "precision_dt=[]\n",
    "\n",
    "#recall\n",
    "recall_dt=[]\n",
    "\n",
    "#f1 score\n",
    "f1_score_dt=[]\n",
    "\n",
    "# result  matrix\n",
    "d=[]\n",
    "result = [[0,0],[0,0]]\n",
    "\n",
    "#Splitting out features and label dataset into train and test sets\n",
    "kf_ = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "#Creating Decision Tree Classifier\n",
    "d_tree = DecisionTreeClassifier()\n",
    "\n",
    "for train_index, test_index in kf_.split(PCA_Feature_Matrix):\n",
    "    X_train_dt, X_test_dt = PCA_Feature_Matrix[train_index], PCA_Feature_Matrix[test_index]\n",
    "    Y_train_dt, Y_test_dt = label_M_[train_index], label_M_[test_index]\n",
    "    \n",
    "    #Training model using the train split data from above\n",
    "    d_tree.fit(X_train_dt,Y_train_dt)\n",
    "    preds = d_tree.predict(X_test_dt)\n",
    "    \n",
    "    d=confusion_matrix(Y_test_dt, preds)\n",
    "    \n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            result[i][j] = (result[i][j] + d[i][j])\n",
    "    # Precision \n",
    "    a=precision_score(Y_test_dt, preds)\n",
    "    precision_dt.append(a)\n",
    "    \n",
    "    # Recall\n",
    "    b=recall_score(Y_test_dt, preds)\n",
    "    recall_dt.append(b)\n",
    "\n",
    "    # F1 score\n",
    "    c=f1_score(Y_test_dt, preds)\n",
    "    f1_score_dt.append(c)\n",
    "    \n",
    "    #Pickle Code Starts\n",
    "    d_tree_file = 'd_tree_model.sav'\n",
    "    pickle.dump(d_tree,open(d_tree_file,'wb'))\n",
    "    score_dt.append(d_tree.score(X_test_dt,Y_test_dt))\n",
    "\n",
    "Final_Accuracy_dt = np.max(score_dt)\n",
    "Final_precision_dt=np.max(precision_dt)\n",
    "Final_recall_dt=np.max(recall_dt)\n",
    "Final_f1_score_dt=np.max(f1_score_dt)\n",
    "print(result)\n",
    "print(\"Accuracy : %s\"%(Final_Accuracy_dt))  \n",
    "print(\"Precision : %s\"%(Final_precision_dt))\n",
    "print(\"Recall : %s\"%(Final_recall_dt))\n",
    "print(\"F1 score : %s\"%(Final_f1_score_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[166, 79], [92, 157]]\n",
      "Accuracy : 0.78\n",
      "Precision : 0.9047619047619048\n",
      "Recall : 0.75\n",
      "F1 score : 0.75\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "score_rf = []\n",
    "\n",
    "# confusion matrix\n",
    "confusion_rf=[]\n",
    "\n",
    "#storing precision\n",
    "precision_rf=[]\n",
    "\n",
    "#recall\n",
    "recall_rf=[]\n",
    "\n",
    "#f1 score\n",
    "f1_score_rf=[]\n",
    "\n",
    "# result  matrix\n",
    "d=[]\n",
    "result = [[0,0],[0,0]]\n",
    "\n",
    "kf_ = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "rand_for = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0)\n",
    "\n",
    "for train_index, test_index in kf_.split(PCA_Feature_Matrix):\n",
    "    X_train_rf, X_test_rf = PCA_Feature_Matrix[train_index], PCA_Feature_Matrix[test_index]\n",
    "    Y_train_rf, Y_test_rf = label_M_[train_index], label_M_[test_index]\n",
    "    \n",
    "    #Training model using the train split data from above\n",
    "    rand_for.fit(X_train_rf,Y_train_rf)\n",
    "    preds = rand_for.predict(X_test_rf)\n",
    "    \n",
    "    d=confusion_matrix(Y_test_rf, preds)\n",
    "    \n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            result[i][j] = (result[i][j] + d[i][j])\n",
    "    # Precision \n",
    "    a=precision_score(Y_test_rf, preds)\n",
    "    precision_rf.append(a)\n",
    "    \n",
    "    # Recall\n",
    "    b=recall_score(Y_test_rf, preds)\n",
    "    recall_rf.append(b)\n",
    "\n",
    "    # F1 score\n",
    "    c=f1_score(Y_test_rf, preds)\n",
    "    f1_score_rf.append(c)\n",
    "    \n",
    "    \n",
    "    #Pickle Code Starts\n",
    "    rand_for_file = 'rand_for_model.sav'\n",
    "    pickle.dump(rand_for,open(rand_for_file,'wb'))\n",
    "    \n",
    "    score_rf.append(rand_for.score(X_test_rf,Y_test_rf))\n",
    "\n",
    "Final_Accuracy_rf = np.max(score_rf)\n",
    "Final_precision_rf=np.max(precision_rf)\n",
    "Final_recall_rf=np.max(recall_rf)\n",
    "Final_f1_score_rf=np.max(f1_score_rf)\n",
    "print(result)\n",
    "print(\"Accuracy : %s\"%(Final_Accuracy_rf))  \n",
    "print(\"Precision : %s\"%(Final_precision_rf))\n",
    "print(\"Recall : %s\"%(Final_recall_rf))\n",
    "print(\"F1 score : %s\"%(Final_f1_score_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[170, 75], [98, 151]]\n",
      "Accuracy : 0.76\n",
      "Precision : 0.8421052631578947\n",
      "Recall : 0.7619047619047619\n",
      "F1 score : 0.7333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#Array to store Accuracy for each K FOld testing\n",
    "score_dt = []\n",
    "\n",
    "# confusion matrix\n",
    "confusion_gb=[]\n",
    "\n",
    "#storing precision\n",
    "precision_gb=[]\n",
    "\n",
    "#recall\n",
    "recall_gb=[]\n",
    "\n",
    "#f1 score\n",
    "f1_score_gb=[]\n",
    "\n",
    "# result  matrix\n",
    "d=[]\n",
    "result = [[0,0],[0,0]]\n",
    "\n",
    "#Splitting out features and label dataset into train and test sets\n",
    "kf_ = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "for train_index, test_index in kf_.split(PCA_Feature_Matrix):\n",
    "    X_train_dt, X_test_dt = PCA_Feature_Matrix[train_index], PCA_Feature_Matrix[test_index]\n",
    "    Y_train_dt, Y_test_dt = label_M_[train_index], label_M_[test_index]\n",
    "    \n",
    "    clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1,\n",
    "        max_depth=1, random_state=0).fit(X_train_dt, Y_train_dt)\n",
    "    \n",
    "    preds = clf.predict(X_test_dt)\n",
    "    \n",
    "    d=confusion_matrix(Y_test_dt, preds)\n",
    "    \n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            result[i][j] = (result[i][j] + d[i][j])\n",
    "    # Precision \n",
    "    a=precision_score(Y_test_dt, preds)\n",
    "    precision_gb.append(a)\n",
    "    \n",
    "    # Recall\n",
    "    b=recall_score(Y_test_dt, preds)\n",
    "    recall_gb.append(b)\n",
    "\n",
    "    # F1 score\n",
    "    c=f1_score(Y_test_dt, preds)\n",
    "    f1_score_gb.append(c)\n",
    "    \n",
    "    \n",
    "    #Pickle Code Starts\n",
    "    clf_file = 'clf_model.sav'\n",
    "    pickle.dump(clf,open(clf_file,'wb'))\n",
    "    \n",
    "    score_dt.append(clf.score(X_test_dt, Y_test_dt))\n",
    "    \n",
    "Final_Accuracy_gb = np.max(score_dt)\n",
    "Final_precision_gb=np.max(precision_gb)\n",
    "Final_recall_gb=np.max(recall_gb)\n",
    "Final_f1_score_gb=np.max(f1_score_gb)\n",
    "print(result)\n",
    "print(\"Accuracy : %s\"%(Final_Accuracy_gb))  \n",
    "print(\"Precision : %s\"%(Final_precision_gb))\n",
    "print(\"Recall : %s\"%(Final_recall_gb))\n",
    "print(\"F1 score : %s\"%(Final_f1_score_gb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[161, 84], [91, 158]]\n",
      "Accuracy : 0.7346938775510204\n",
      "Precision : 0.7692307692307693\n",
      "Recall : 0.8095238095238095\n",
      "F1 score : 0.7547169811320754\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "#Array to store Accuracy for each K FOld testing\n",
    "score_dt1 = []\n",
    "\n",
    "# confusion matrix\n",
    "confusion_ab=[]\n",
    "\n",
    "#storing precision\n",
    "precision_ab=[]\n",
    "\n",
    "#recall\n",
    "recall_ab=[]\n",
    "\n",
    "#f1 score\n",
    "f1_score_ab=[]\n",
    "\n",
    "# result  matrix\n",
    "d=[]\n",
    "result = [[0,0],[0,0]]\n",
    "\n",
    "#Splitting out features and label dataset into train and test sets\n",
    "kf_1 = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "clf1 = AdaBoostClassifier(n_estimators=50, random_state=0)\n",
    "\n",
    "for train_index, test_index in kf_.split(PCA_Feature_Matrix):\n",
    "    X_train_dt, X_test_dt = PCA_Feature_Matrix[train_index], PCA_Feature_Matrix[test_index]\n",
    "    Y_train_dt, Y_test_dt = label_M_[train_index], label_M_[test_index]\n",
    "    \n",
    "    #Training model using the train split data from above\n",
    "    clf1.fit(X_train_dt,Y_train_dt)\n",
    "    preds = clf1.predict(X_test_dt)\n",
    "    \n",
    "    d=confusion_matrix(Y_test_dt, preds)\n",
    "    \n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            result[i][j] = (result[i][j] + d[i][j])\n",
    "    # Precision \n",
    "    a=precision_score(Y_test_dt, preds)\n",
    "    precision_ab.append(a)\n",
    "    \n",
    "    # Recall\n",
    "    b=recall_score(Y_test_dt, preds)\n",
    "    recall_ab.append(b)\n",
    "\n",
    "    # F1 score\n",
    "    c=f1_score(Y_test_dt, preds)\n",
    "    f1_score_ab.append(c)\n",
    "    \n",
    "    \n",
    "    #Pickle Code Starts\n",
    "    clf1_file = 'clf1_model.sav'\n",
    "    pickle.dump(clf1,open(clf1_file,'wb'))\n",
    "    \n",
    "    score_dt1.append(clf1.score(X_test_dt,Y_test_dt))\n",
    "\n",
    "Final_Accuracy_ab = np.max(score_dt1)\n",
    "Final_precision_ab=np.max(precision_ab)\n",
    "Final_recall_ab=np.max(recall_ab)\n",
    "Final_f1_score_ab=np.max(f1_score_ab)\n",
    "print(result)\n",
    "print(\"Accuracy : %s\"%(Final_Accuracy_ab))  \n",
    "print(\"Precision : %s\"%(Final_precision_ab))\n",
    "print(\"Recall : %s\"%(Final_recall_ab))\n",
    "print(\"F1 score : %s\"%(Final_f1_score_ab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 494 samples\n",
      "Epoch 1/200\n",
      "494/494 [==============================] - 0s 575us/sample - loss: 0.6956 - accuracy: 0.4636\n",
      "Epoch 2/200\n",
      "494/494 [==============================] - 0s 129us/sample - loss: 0.6871 - accuracy: 0.5729\n",
      "Epoch 3/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.6813 - accuracy: 0.6032\n",
      "Epoch 4/200\n",
      "494/494 [==============================] - 0s 129us/sample - loss: 0.6750 - accuracy: 0.6134\n",
      "Epoch 5/200\n",
      "494/494 [==============================] - 0s 105us/sample - loss: 0.6678 - accuracy: 0.6174\n",
      "Epoch 6/200\n",
      "494/494 [==============================] - 0s 129us/sample - loss: 0.6601 - accuracy: 0.6275\n",
      "Epoch 7/200\n",
      "494/494 [==============================] - 0s 105us/sample - loss: 0.6523 - accuracy: 0.6296\n",
      "Epoch 8/200\n",
      "494/494 [==============================] - 0s 129us/sample - loss: 0.6439 - accuracy: 0.6296\n",
      "Epoch 9/200\n",
      "494/494 [==============================] - 0s 113us/sample - loss: 0.6360 - accuracy: 0.6397\n",
      "Epoch 10/200\n",
      "494/494 [==============================] - 0s 129us/sample - loss: 0.6291 - accuracy: 0.6316\n",
      "Epoch 11/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.6224 - accuracy: 0.6397\n",
      "Epoch 12/200\n",
      "494/494 [==============================] - 0s 113us/sample - loss: 0.6177 - accuracy: 0.6518\n",
      "Epoch 13/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.6128 - accuracy: 0.6518\n",
      "Epoch 14/200\n",
      "494/494 [==============================] - 0s 105us/sample - loss: 0.6089 - accuracy: 0.6579\n",
      "Epoch 15/200\n",
      "494/494 [==============================] - 0s 129us/sample - loss: 0.6049 - accuracy: 0.6579\n",
      "Epoch 16/200\n",
      "494/494 [==============================] - 0s 119us/sample - loss: 0.6015 - accuracy: 0.6599\n",
      "Epoch 17/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.5981 - accuracy: 0.6640\n",
      "Epoch 18/200\n",
      "494/494 [==============================] - 0s 140us/sample - loss: 0.5947 - accuracy: 0.6538\n",
      "Epoch 19/200\n",
      "494/494 [==============================] - 0s 124us/sample - loss: 0.5915 - accuracy: 0.6721\n",
      "Epoch 20/200\n",
      "494/494 [==============================] - 0s 115us/sample - loss: 0.5887 - accuracy: 0.6862\n",
      "Epoch 21/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.5858 - accuracy: 0.6802\n",
      "Epoch 22/200\n",
      "494/494 [==============================] - 0s 129us/sample - loss: 0.5842 - accuracy: 0.6883\n",
      "Epoch 23/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.5811 - accuracy: 0.6883\n",
      "Epoch 24/200\n",
      "494/494 [==============================] - 0s 129us/sample - loss: 0.5802 - accuracy: 0.6903\n",
      "Epoch 25/200\n",
      "494/494 [==============================] - 0s 113us/sample - loss: 0.5774 - accuracy: 0.6964\n",
      "Epoch 26/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.5777 - accuracy: 0.6842\n",
      "Epoch 27/200\n",
      "494/494 [==============================] - 0s 113us/sample - loss: 0.5742 - accuracy: 0.7065\n",
      "Epoch 28/200\n",
      "494/494 [==============================] - 0s 113us/sample - loss: 0.5720 - accuracy: 0.6802\n",
      "Epoch 29/200\n",
      "494/494 [==============================] - 0s 146us/sample - loss: 0.5696 - accuracy: 0.7105\n",
      "Epoch 30/200\n",
      "494/494 [==============================] - 0s 137us/sample - loss: 0.5694 - accuracy: 0.7085\n",
      "Epoch 31/200\n",
      "494/494 [==============================] - 0s 137us/sample - loss: 0.5666 - accuracy: 0.7126\n",
      "Epoch 32/200\n",
      "494/494 [==============================] - 0s 170us/sample - loss: 0.5648 - accuracy: 0.7085\n",
      "Epoch 33/200\n",
      "494/494 [==============================] - 0s 162us/sample - loss: 0.5633 - accuracy: 0.7065\n",
      "Epoch 34/200\n",
      "494/494 [==============================] - 0s 113us/sample - loss: 0.5626 - accuracy: 0.7085\n",
      "Epoch 35/200\n",
      "494/494 [==============================] - 0s 113us/sample - loss: 0.5600 - accuracy: 0.7126\n",
      "Epoch 36/200\n",
      "494/494 [==============================] - 0s 119us/sample - loss: 0.5592 - accuracy: 0.7085\n",
      "Epoch 37/200\n",
      "494/494 [==============================] - 0s 105us/sample - loss: 0.5575 - accuracy: 0.7105\n",
      "Epoch 38/200\n",
      "494/494 [==============================] - 0s 129us/sample - loss: 0.5564 - accuracy: 0.7126\n",
      "Epoch 39/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.5550 - accuracy: 0.7105\n",
      "Epoch 40/200\n",
      "494/494 [==============================] - 0s 135us/sample - loss: 0.5532 - accuracy: 0.7186\n",
      "Epoch 41/200\n",
      "494/494 [==============================] - 0s 146us/sample - loss: 0.5519 - accuracy: 0.7146\n",
      "Epoch 42/200\n",
      "494/494 [==============================] - 0s 116us/sample - loss: 0.5506 - accuracy: 0.7186\n",
      "Epoch 43/200\n",
      "494/494 [==============================] - 0s 124us/sample - loss: 0.5500 - accuracy: 0.7206\n",
      "Epoch 44/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.5490 - accuracy: 0.7166\n",
      "Epoch 45/200\n",
      "494/494 [==============================] - 0s 132us/sample - loss: 0.5483 - accuracy: 0.7166\n",
      "Epoch 46/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.5470 - accuracy: 0.7247\n",
      "Epoch 47/200\n",
      "494/494 [==============================] - 0s 137us/sample - loss: 0.5446 - accuracy: 0.7186\n",
      "Epoch 48/200\n",
      "494/494 [==============================] - 0s 113us/sample - loss: 0.5442 - accuracy: 0.7227\n",
      "Epoch 49/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.5431 - accuracy: 0.7267\n",
      "Epoch 50/200\n",
      "494/494 [==============================] - 0s 105us/sample - loss: 0.5419 - accuracy: 0.7267\n",
      "Epoch 51/200\n",
      "494/494 [==============================] - 0s 113us/sample - loss: 0.5408 - accuracy: 0.7267\n",
      "Epoch 52/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.5401 - accuracy: 0.7267\n",
      "Epoch 53/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.5383 - accuracy: 0.7247\n",
      "Epoch 54/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.5376 - accuracy: 0.7126\n",
      "Epoch 55/200\n",
      "494/494 [==============================] - 0s 129us/sample - loss: 0.5379 - accuracy: 0.7186\n",
      "Epoch 56/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.5352 - accuracy: 0.7247\n",
      "Epoch 57/200\n",
      "494/494 [==============================] - 0s 129us/sample - loss: 0.5349 - accuracy: 0.7247\n",
      "Epoch 58/200\n",
      "494/494 [==============================] - 0s 113us/sample - loss: 0.5330 - accuracy: 0.7247\n",
      "Epoch 59/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.5321 - accuracy: 0.7206\n",
      "Epoch 60/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.5312 - accuracy: 0.7267\n",
      "Epoch 61/200\n",
      "494/494 [==============================] - 0s 129us/sample - loss: 0.5316 - accuracy: 0.7247\n",
      "Epoch 62/200\n",
      "494/494 [==============================] - 0s 131us/sample - loss: 0.5294 - accuracy: 0.7308\n",
      "Epoch 63/200\n",
      "494/494 [==============================] - 0s 129us/sample - loss: 0.5291 - accuracy: 0.7267\n",
      "Epoch 64/200\n",
      "494/494 [==============================] - 0s 140us/sample - loss: 0.5280 - accuracy: 0.7206\n",
      "Epoch 65/200\n",
      "494/494 [==============================] - 0s 137us/sample - loss: 0.5262 - accuracy: 0.7287\n",
      "Epoch 66/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.5261 - accuracy: 0.7227\n",
      "Epoch 67/200\n",
      "494/494 [==============================] - 0s 129us/sample - loss: 0.5254 - accuracy: 0.7267\n",
      "Epoch 68/200\n",
      "494/494 [==============================] - 0s 129us/sample - loss: 0.5241 - accuracy: 0.7267\n",
      "Epoch 69/200\n",
      "494/494 [==============================] - 0s 129us/sample - loss: 0.5225 - accuracy: 0.7308 - loss: 0.5127 - accuracy: 0.73\n",
      "Epoch 70/200\n",
      "494/494 [==============================] - 0s 125us/sample - loss: 0.5214 - accuracy: 0.7348\n",
      "Epoch 71/200\n",
      "494/494 [==============================] - 0s 148us/sample - loss: 0.5204 - accuracy: 0.7287\n",
      "Epoch 72/200\n",
      "494/494 [==============================] - 0s 137us/sample - loss: 0.5193 - accuracy: 0.7308\n",
      "Epoch 73/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.5183 - accuracy: 0.7368\n",
      "Epoch 74/200\n",
      "494/494 [==============================] - 0s 129us/sample - loss: 0.5180 - accuracy: 0.7409\n",
      "Epoch 75/200\n",
      "494/494 [==============================] - 0s 129us/sample - loss: 0.5170 - accuracy: 0.7287\n",
      "Epoch 76/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "494/494 [==============================] - 0s 129us/sample - loss: 0.5157 - accuracy: 0.7368\n",
      "Epoch 77/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.5141 - accuracy: 0.7389\n",
      "Epoch 78/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.5135 - accuracy: 0.7368\n",
      "Epoch 79/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.5131 - accuracy: 0.7389\n",
      "Epoch 80/200\n",
      "494/494 [==============================] - 0s 113us/sample - loss: 0.5111 - accuracy: 0.7429\n",
      "Epoch 81/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.5101 - accuracy: 0.7389\n",
      "Epoch 82/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.5100 - accuracy: 0.7368\n",
      "Epoch 83/200\n",
      "494/494 [==============================] - 0s 117us/sample - loss: 0.5076 - accuracy: 0.7449\n",
      "Epoch 84/200\n",
      "494/494 [==============================] - 0s 137us/sample - loss: 0.5077 - accuracy: 0.7389\n",
      "Epoch 85/200\n",
      "494/494 [==============================] - 0s 123us/sample - loss: 0.5056 - accuracy: 0.7449\n",
      "Epoch 86/200\n",
      "494/494 [==============================] - 0s 129us/sample - loss: 0.5063 - accuracy: 0.7409\n",
      "Epoch 87/200\n",
      "494/494 [==============================] - 0s 129us/sample - loss: 0.5062 - accuracy: 0.7470\n",
      "Epoch 88/200\n",
      "494/494 [==============================] - 0s 129us/sample - loss: 0.5048 - accuracy: 0.7470\n",
      "Epoch 89/200\n",
      "494/494 [==============================] - 0s 129us/sample - loss: 0.5030 - accuracy: 0.7470\n",
      "Epoch 90/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.5013 - accuracy: 0.7429\n",
      "Epoch 91/200\n",
      "494/494 [==============================] - 0s 113us/sample - loss: 0.5004 - accuracy: 0.7409\n",
      "Epoch 92/200\n",
      "494/494 [==============================] - 0s 129us/sample - loss: 0.5000 - accuracy: 0.7510\n",
      "Epoch 93/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.4988 - accuracy: 0.7470\n",
      "Epoch 94/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.4984 - accuracy: 0.7470\n",
      "Epoch 95/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.4969 - accuracy: 0.7470\n",
      "Epoch 96/200\n",
      "494/494 [==============================] - 0s 129us/sample - loss: 0.4979 - accuracy: 0.7429\n",
      "Epoch 97/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.4950 - accuracy: 0.7510\n",
      "Epoch 98/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.4945 - accuracy: 0.7449\n",
      "Epoch 99/200\n",
      "494/494 [==============================] - 0s 113us/sample - loss: 0.4939 - accuracy: 0.7530\n",
      "Epoch 100/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.4922 - accuracy: 0.7470\n",
      "Epoch 101/200\n",
      "494/494 [==============================] - 0s 134us/sample - loss: 0.4927 - accuracy: 0.7530\n",
      "Epoch 102/200\n",
      "494/494 [==============================] - 0s 129us/sample - loss: 0.4909 - accuracy: 0.7470\n",
      "Epoch 103/200\n",
      "494/494 [==============================] - 0s 113us/sample - loss: 0.4894 - accuracy: 0.7551\n",
      "Epoch 104/200\n",
      "494/494 [==============================] - 0s 134us/sample - loss: 0.4891 - accuracy: 0.7551\n",
      "Epoch 105/200\n",
      "494/494 [==============================] - 0s 134us/sample - loss: 0.4872 - accuracy: 0.7571\n",
      "Epoch 106/200\n",
      "494/494 [==============================] - 0s 124us/sample - loss: 0.4866 - accuracy: 0.7551\n",
      "Epoch 107/200\n",
      "494/494 [==============================] - 0s 138us/sample - loss: 0.4856 - accuracy: 0.7611\n",
      "Epoch 108/200\n",
      "494/494 [==============================] - 0s 129us/sample - loss: 0.4856 - accuracy: 0.7530\n",
      "Epoch 109/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.4839 - accuracy: 0.7571\n",
      "Epoch 110/200\n",
      "494/494 [==============================] - 0s 113us/sample - loss: 0.4839 - accuracy: 0.7591\n",
      "Epoch 111/200\n",
      "494/494 [==============================] - 0s 129us/sample - loss: 0.4821 - accuracy: 0.7632\n",
      "Epoch 112/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.4819 - accuracy: 0.7571\n",
      "Epoch 113/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.4809 - accuracy: 0.7632\n",
      "Epoch 114/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.4810 - accuracy: 0.7652\n",
      "Epoch 115/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.4787 - accuracy: 0.7632\n",
      "Epoch 116/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.4784 - accuracy: 0.7632\n",
      "Epoch 117/200\n",
      "494/494 [==============================] - 0s 137us/sample - loss: 0.4775 - accuracy: 0.7591\n",
      "Epoch 118/200\n",
      "494/494 [==============================] - 0s 129us/sample - loss: 0.4772 - accuracy: 0.7672\n",
      "Epoch 119/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.4753 - accuracy: 0.7591\n",
      "Epoch 120/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.4753 - accuracy: 0.7611\n",
      "Epoch 121/200\n",
      "494/494 [==============================] - 0s 137us/sample - loss: 0.4735 - accuracy: 0.7591\n",
      "Epoch 122/200\n",
      "494/494 [==============================] - 0s 142us/sample - loss: 0.4730 - accuracy: 0.7632\n",
      "Epoch 123/200\n",
      "494/494 [==============================] - 0s 137us/sample - loss: 0.4742 - accuracy: 0.7632\n",
      "Epoch 124/200\n",
      "494/494 [==============================] - 0s 178us/sample - loss: 0.4723 - accuracy: 0.7591\n",
      "Epoch 125/200\n",
      "494/494 [==============================] - 0s 170us/sample - loss: 0.4709 - accuracy: 0.7632\n",
      "Epoch 126/200\n",
      "494/494 [==============================] - 0s 137us/sample - loss: 0.4689 - accuracy: 0.7632\n",
      "Epoch 127/200\n",
      "494/494 [==============================] - 0s 170us/sample - loss: 0.4694 - accuracy: 0.7611\n",
      "Epoch 128/200\n",
      "494/494 [==============================] - 0s 170us/sample - loss: 0.4693 - accuracy: 0.7672\n",
      "Epoch 129/200\n",
      "494/494 [==============================] - 0s 129us/sample - loss: 0.4676 - accuracy: 0.7713\n",
      "Epoch 130/200\n",
      "494/494 [==============================] - 0s 137us/sample - loss: 0.4663 - accuracy: 0.7591\n",
      "Epoch 131/200\n",
      "494/494 [==============================] - 0s 129us/sample - loss: 0.4660 - accuracy: 0.7611\n",
      "Epoch 132/200\n",
      "494/494 [==============================] - 0s 129us/sample - loss: 0.4654 - accuracy: 0.7692\n",
      "Epoch 133/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.4656 - accuracy: 0.7672\n",
      "Epoch 134/200\n",
      "494/494 [==============================] - 0s 113us/sample - loss: 0.4645 - accuracy: 0.7632\n",
      "Epoch 135/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.4622 - accuracy: 0.7611\n",
      "Epoch 136/200\n",
      "494/494 [==============================] - 0s 129us/sample - loss: 0.4618 - accuracy: 0.7692\n",
      "Epoch 137/200\n",
      "494/494 [==============================] - 0s 137us/sample - loss: 0.4606 - accuracy: 0.7733\n",
      "Epoch 138/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.4606 - accuracy: 0.7713\n",
      "Epoch 139/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.4603 - accuracy: 0.7794\n",
      "Epoch 140/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.4591 - accuracy: 0.7652\n",
      "Epoch 141/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.4585 - accuracy: 0.7692\n",
      "Epoch 142/200\n",
      "494/494 [==============================] - 0s 129us/sample - loss: 0.4592 - accuracy: 0.7733\n",
      "Epoch 143/200\n",
      "494/494 [==============================] - 0s 129us/sample - loss: 0.4566 - accuracy: 0.7692\n",
      "Epoch 144/200\n",
      "494/494 [==============================] - 0s 137us/sample - loss: 0.4566 - accuracy: 0.7692\n",
      "Epoch 145/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.4568 - accuracy: 0.7753\n",
      "Epoch 146/200\n",
      "494/494 [==============================] - 0s 129us/sample - loss: 0.4538 - accuracy: 0.7753\n",
      "Epoch 147/200\n",
      "494/494 [==============================] - 0s 117us/sample - loss: 0.4538 - accuracy: 0.7753\n",
      "Epoch 148/200\n",
      "494/494 [==============================] - 0s 129us/sample - loss: 0.4541 - accuracy: 0.7814\n",
      "Epoch 149/200\n",
      "494/494 [==============================] - 0s 132us/sample - loss: 0.4520 - accuracy: 0.7814\n",
      "Epoch 150/200\n",
      "494/494 [==============================] - 0s 137us/sample - loss: 0.4521 - accuracy: 0.7854\n",
      "Epoch 151/200\n",
      "494/494 [==============================] - 0s 123us/sample - loss: 0.4496 - accuracy: 0.7773\n",
      "Epoch 152/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "494/494 [==============================] - 0s 137us/sample - loss: 0.4499 - accuracy: 0.7733\n",
      "Epoch 153/200\n",
      "494/494 [==============================] - 0s 154us/sample - loss: 0.4504 - accuracy: 0.7834\n",
      "Epoch 154/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.4490 - accuracy: 0.7794\n",
      "Epoch 155/200\n",
      "494/494 [==============================] - 0s 105us/sample - loss: 0.4483 - accuracy: 0.7773\n",
      "Epoch 156/200\n",
      "494/494 [==============================] - 0s 146us/sample - loss: 0.4480 - accuracy: 0.7773\n",
      "Epoch 157/200\n",
      "494/494 [==============================] - 0s 137us/sample - loss: 0.4486 - accuracy: 0.7854\n",
      "Epoch 158/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.4462 - accuracy: 0.7895\n",
      "Epoch 159/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.4474 - accuracy: 0.7834\n",
      "Epoch 160/200\n",
      "494/494 [==============================] - 0s 113us/sample - loss: 0.4469 - accuracy: 0.7874\n",
      "Epoch 161/200\n",
      "494/494 [==============================] - 0s 129us/sample - loss: 0.4442 - accuracy: 0.7895\n",
      "Epoch 162/200\n",
      "494/494 [==============================] - 0s 137us/sample - loss: 0.4455 - accuracy: 0.7895\n",
      "Epoch 163/200\n",
      "494/494 [==============================] - 0s 137us/sample - loss: 0.4442 - accuracy: 0.7895\n",
      "Epoch 164/200\n",
      "494/494 [==============================] - 0s 129us/sample - loss: 0.4421 - accuracy: 0.7895\n",
      "Epoch 165/200\n",
      "494/494 [==============================] - 0s 129us/sample - loss: 0.4418 - accuracy: 0.7874\n",
      "Epoch 166/200\n",
      "494/494 [==============================] - 0s 132us/sample - loss: 0.4432 - accuracy: 0.7814\n",
      "Epoch 167/200\n",
      "494/494 [==============================] - 0s 137us/sample - loss: 0.4421 - accuracy: 0.7814\n",
      "Epoch 168/200\n",
      "494/494 [==============================] - 0s 148us/sample - loss: 0.4420 - accuracy: 0.7874\n",
      "Epoch 169/200\n",
      "494/494 [==============================] - 0s 146us/sample - loss: 0.4403 - accuracy: 0.7874\n",
      "Epoch 170/200\n",
      "494/494 [==============================] - 0s 137us/sample - loss: 0.4404 - accuracy: 0.7834\n",
      "Epoch 171/200\n",
      "494/494 [==============================] - 0s 137us/sample - loss: 0.4404 - accuracy: 0.7874\n",
      "Epoch 172/200\n",
      "494/494 [==============================] - 0s 129us/sample - loss: 0.4388 - accuracy: 0.7854\n",
      "Epoch 173/200\n",
      "494/494 [==============================] - 0s 129us/sample - loss: 0.4383 - accuracy: 0.7874\n",
      "Epoch 174/200\n",
      "494/494 [==============================] - 0s 137us/sample - loss: 0.4377 - accuracy: 0.7935\n",
      "Epoch 175/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.4371 - accuracy: 0.7895\n",
      "Epoch 176/200\n",
      "494/494 [==============================] - 0s 129us/sample - loss: 0.4363 - accuracy: 0.7976\n",
      "Epoch 177/200\n",
      "494/494 [==============================] - 0s 129us/sample - loss: 0.4354 - accuracy: 0.7874\n",
      "Epoch 178/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.4349 - accuracy: 0.7935\n",
      "Epoch 179/200\n",
      "494/494 [==============================] - 0s 137us/sample - loss: 0.4349 - accuracy: 0.7895\n",
      "Epoch 180/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.4342 - accuracy: 0.7874\n",
      "Epoch 181/200\n",
      "494/494 [==============================] - 0s 129us/sample - loss: 0.4352 - accuracy: 0.7874\n",
      "Epoch 182/200\n",
      "494/494 [==============================] - 0s 121us/sample - loss: 0.4332 - accuracy: 0.7895\n",
      "Epoch 183/200\n",
      "494/494 [==============================] - 0s 134us/sample - loss: 0.4324 - accuracy: 0.7935\n",
      "Epoch 184/200\n",
      "494/494 [==============================] - 0s 130us/sample - loss: 0.4331 - accuracy: 0.7874\n",
      "Epoch 185/200\n",
      "494/494 [==============================] - 0s 138us/sample - loss: 0.4333 - accuracy: 0.7935\n",
      "Epoch 186/200\n",
      "494/494 [==============================] - 0s 135us/sample - loss: 0.4313 - accuracy: 0.7895\n",
      "Epoch 187/200\n",
      "494/494 [==============================] - 0s 137us/sample - loss: 0.4332 - accuracy: 0.7895\n",
      "Epoch 188/200\n",
      "494/494 [==============================] - 0s 132us/sample - loss: 0.4322 - accuracy: 0.7935\n",
      "Epoch 189/200\n",
      "494/494 [==============================] - 0s 140us/sample - loss: 0.4298 - accuracy: 0.7895\n",
      "Epoch 190/200\n",
      "494/494 [==============================] - 0s 129us/sample - loss: 0.4298 - accuracy: 0.7935\n",
      "Epoch 191/200\n",
      "494/494 [==============================] - 0s 123us/sample - loss: 0.4299 - accuracy: 0.7915\n",
      "Epoch 192/200\n",
      "494/494 [==============================] - 0s 129us/sample - loss: 0.4283 - accuracy: 0.7955\n",
      "Epoch 193/200\n",
      "494/494 [==============================] - 0s 129us/sample - loss: 0.4285 - accuracy: 0.7996\n",
      "Epoch 194/200\n",
      "494/494 [==============================] - 0s 129us/sample - loss: 0.4293 - accuracy: 0.7955\n",
      "Epoch 195/200\n",
      "494/494 [==============================] - 0s 129us/sample - loss: 0.4277 - accuracy: 0.7976\n",
      "Epoch 196/200\n",
      "494/494 [==============================] - 0s 154us/sample - loss: 0.4285 - accuracy: 0.8016\n",
      "Epoch 197/200\n",
      "494/494 [==============================] - 0s 137us/sample - loss: 0.4272 - accuracy: 0.8036\n",
      "Epoch 198/200\n",
      "494/494 [==============================] - 0s 154us/sample - loss: 0.4277 - accuracy: 0.7976\n",
      "Epoch 199/200\n",
      "494/494 [==============================] - 0s 146us/sample - loss: 0.4273 - accuracy: 0.8016\n",
      "Epoch 200/200\n",
      "494/494 [==============================] - 0s 129us/sample - loss: 0.4253 - accuracy: 0.7935\n",
      "494/1 [====================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 495us/sample - loss: 0.3642 - accuracy: 0.7976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 79.76\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPool2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=10, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit the keras model on the dataset\n",
    "model.fit(pd.DataFrame(PCA_Feature_Matrix), pd.DataFrame(label_M_),shuffle=True, epochs=200, batch_size=10)\n",
    "# evaluate the keras model\n",
    "model.save('ann_model.h5')\n",
    "_, accuracy = model.evaluate(Final_Feature_M, label_M_)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
